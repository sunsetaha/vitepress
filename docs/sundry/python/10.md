# 爬取猫眼电影的Top100

https://maoyan.com/board/4 电影名称、上映时间、主演信息

在开始编写程序之前，首先要确定页面类型（静态页面或动态页面），其次找出页面的 url 规律，最后通过分析网页元素结构来确定正则表达式，从而提取网页信息。

## 页面类型

点击右键查看页面源码，确定要抓取的数据是否存在于页面内。通过浏览得知要抓取的信息全部存在于源码内，因此该页面输属于静态页面。
```html
<div class="movie-item-info">
    <p class="name"><a href="/films/1200486" title="我不是药神" data-act="boarditem-click" data-val="{movieId:1200486}">我不是药神</a></p>
    <p class="star">
            主演：徐峥,周一围,王传君
    </p>
    <p class="releasetime">上映时间：2018-07-05</p>    
</div>
```

## 确定URL规律

```
第一页：https://maoyan.com/board/4?offset=0
第二页：https://maoyan.com/board/4?offset=10
第三页：https://maoyan.com/board/4?offset=20
...
第n页：https://maoyan.com/board/4?offset=(n-1)*10
```

## 确定正则表达式

```regexp
<div class="movie-item-info">.*?title="(.*?)".*?class="star">(.*?)</p>.*?releasetime">(.*?)</p>
```
编写正则表达式时将需要提取的信息使用(.*?)代替，而不需要的内容（包括元素标签）使用.*?代替。

## 编写程序

```python
   from urllib import request
   import re
   import time
   import random
   import csv
   from ua_info import ua_list
   
   # 定义一个爬虫类
   class MaoyanSpider(Object):
       # 初始化
       def __init__(self):
           self.url = 'https://maoyan.com/board/4?offset={}'
           
       # 请求函数
       def get_html(self, url):
           headers = {'User-Agent': random.choice(ua_list)}
           req = request.Request(url=url, headers=headers)
           response = request.urlopen(req)
           return response.read().decode()
           parse_html(self, html):
       
       # 解析函数
       def parse_html(self, html):
           # 正则表达式对象
           pattern = re.compile('<div class="movie-item-info">.*?title="(.*?)".*?class="star">(.*?)</p>.*?releasetime">(.*?)</p>', re.S)
           # regex.findall()返回的是一个列表，列表中的每个元素都是一个元组，元组中的每个元素都是匹配到的内容。
           r_list = pattern.findall(html)
           self.write_csv(r_list)
           
       # 保存数据函数
       def write_csv(self, r_list):
           with open('猫眼电影Top100.csv', 'a', newline='', encoding='utf-8') as f:
               writer = csv.writer(f)
               for r in r_list:
                   print(r)
                   name = r[0].strip()
                   star = r[1].strip()
                   time = r[2].strip()[5:15]
                   L = [name, star, time]
                   writer.writerow(L)
                   print(L)
                   
       # 主函数
       def run(self):
           for i in range(0,11,10):
               url = self.url.format(i)
               html = self.get_html(url)
               time.sleep(random.uniform(1,2))
               
# 启动
if __name__ == '__main__':
    try:
        spider = MaoyanSpider()
        spider.run()
    except Exception as e:
        print(e)  
```
